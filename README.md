# FedTime: A Federated Large Language Model for Long-Term Time Series Forecasting

<div align="center">

[![arxiv](https://img.shields.io/badge/arXiv-2407.20503-b31b1b.svg)](https://openreview.net/pdf?id=ICobFdH8w2)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/release/python-380/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.12+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>

---

##  **Overview**

**FedTime** addresses critical challenges in centralized time series forecasting by introducing an **End-to-end federated learning framework** that leverages Large Language Models for **privacy-preserving**, **collaborative** time series prediction across distributed edge devices.

<div align="center">
<img src="figures/fedtime_framework.png" alt="FedTime Federated Framework" width="800"/>
<p><em>Figure 1: FedTime federated learning architecture with K-means clustering for EV charging stations</em></p>
</div>

###  **Key Contributions**

- **Federated LLM Framework** for time series forecasting
- ğŸ¯ **K-means Clustering** for intelligent client grouping and focused training
- ğŸ”§ **Parameter-Efficient Fine-tuning** (QLoRA) reducing communication by **80%**
- ğŸ“Š **Superior Performance** with up to **20% improvement** over state-of-the-art
- ğŸ”’ **Privacy Preservation** keeping sensitive data on edge devices
- âš¡ **3Ã— Faster Convergence** compared to centralized approaches

---

##  **Performance Highlights**

<div align="center">
<img src="figures/forecast_performance.png" alt="Performance Comparison" width="700"/>
<p><em>Figure 2: FedTime performance across different look-back window lengths L âˆˆ {24, 48, 96, 192, 336, 720}</em></p>
</div>

### ğŸ“Š **Quantitative Results**

| Dataset | Method | MSE (T=720) | MAE (T=720) | **Improvement** |
|---------|---------|:-----------:|:-----------:|:---------------:|
| **Traffic** | LLM4TS | 0.437 | 0.292 | - |
| | **FedTime** | **0.369** | **0.239** | **ğŸ”¥ 15.56%** â†“ |
| **Electricity** | LLM4TS | 0.220 | 0.292 | - |
| | **FedTime** | **0.176** | **0.288** | **ğŸ”¥ 20.0%** â†“ |
| **ETTm1** | LLM4TS | 0.408 | 0.419 | - |
| | **FedTime** | **0.328** | **0.373** | **ğŸ”¥ 10.98%** â†“ |

---

## ğŸ”§ **Model Architecture**

<div align="center">
<img src="figures/llm_architecture.png" alt="FedTime Model Architecture" width="750"/>
<p><em>Figure 3: FedTime two-phase fine-tuning strategy with (a) Supervised fine-tuning and (b) Forecasting fine-tuning</em></p>
</div>

### ğŸ—ï¸ **Core Components**

1. **ğŸ”„ Channel Independence**: Processes each time series variable separately for better feature preservation
2. **ğŸ“¦ Patching Strategy**: Divides time series into patches for efficient LLM processing  
3. **ğŸ§  LLaMA-2 Backbone**: Leverages pre-trained LLaMA-2-7B with 7B parameters
4. **âš¡ QLoRA Fine-tuning**: Parameter-efficient adaptation with only **1.2% trainable parameters**
5. **ğŸ¯ Direct Preference Optimization**: Aligns model behavior with time series patterns
6. **ğŸ¤ Federated Aggregation**: Secure model update combination across clients

---

## ğŸ“ˆ **Experimental Validation**

### ğŸ¯ **Ablation Study Results**

<div align="center">
<img src="figures/actual_predicted.png" alt="Ablation Study Results" width="600"/>
<p><em>Figure 4: Ablation study showing impact of different FedTime components on Caltech EV charging data</em></p>
</div>

**Key Findings:**
- âœ… **Clustering + PEFT** provides optimal performance-efficiency trade-off
- âœ… **PEFT alone** significantly reduces communication overhead  
- âœ… **Clustering alone** improves model personalization

### ğŸ“Š **Client Clustering Analysis**

<div align="center">

| **Before Clustering** | **After Clustering** |
|:---------------------:|:-------------------:|
| <img src="figures/no_clustering.png" alt="Before Clustering" width="350"/> | <img src="figures/with_clustering.png" alt="After Clustering" width="350"/> |

<p><em>Figure 5: K-means clustering visualization showing client distribution based on cluster size and performance metrics</em></p>
</div>

---

##  **Communication Efficiency**

<div align="center">
<img src="figures/communication_overhead.png" alt="Communication Overhead Analysis" width="700"/>
<p><em>Figure 6: Communication overhead comparison showing FedTime's superior efficiency</em></p>
</div>

###  **Efficiency Metrics**

- **ğŸ“‰ 80% Reduction** in communication overhead vs. full fine-tuning
- **âš¡ 3Ã— Faster** convergence than centralized training
- **ğŸ”’ 100% Privacy** preservation with local data processing
- **ğŸ“± Edge-Friendly** deployment on resource-constrained devices

---

##  **Quick Start**

### **Installation**

```bash
# Clone the repository
git clone https://github.com/Jagoul/FedTime-A-Federated-Large-Language-Model-for-Long-Term-Time-Series-Forecasting.git
cd FedTime

# Create conda environment
conda create -n fedtime python=3.8
conda activate fedtime

# Install dependencies
pip install -r requirements.txt
```

### **Dataset Preparation**

```bash
# Download benchmark datasets
bash scripts/download_datasets.sh

# Verify dataset structure
ls dataset/ETT-small/
# Expected: ETTh1.csv ETTh2.csv ETTm1.csv ETTm2.csv
```

### **Training & Evaluation**

####  **Federated Training (Recommended)**
```bash
python run_federated.py \
  --is_training 1 \
  --data ETTh1 \
  --model FedTime \
  --seq_len 96 \
  --pred_len 192 \
  --num_clients 10 \
  --num_rounds 100 \
  --local_epochs 5 \
  --use_clustering 1 \
  --num_clusters 3 \
  --use_peft 1 \
  --peft_method qlora \
  --des 'FedTime_ETTh1'
```

#### ğŸ“Š **Centralized Baseline**
```bash
python run_longExp.py \
  --is_training 1 \
  --data ETTh1 \
  --model FedTime \
  --seq_len 96 \
  --pred_len 192 \
  --des 'Centralized_ETTh1'
```

#### ğŸ” **Communication Analysis**
```bash
python analyze_communication.py \
  --num_clients 10 \
  --num_rounds 100 \
  --output_dir ./communication_analysis
```

---

## ğŸ“ **Repository Structure**

<details>
<summary><b>ğŸ—‚ï¸ Click to expand file structure</b></summary>

```
FedTime/
â”œâ”€â”€ ğŸ“ figures/                 # Paper figures and visualizations
â”‚   â”œâ”€â”€ actual_predicted.png    # Actual vs predicted results
â”‚   â”œâ”€â”€ communication_overhead.pdf # Communication analysis
â”‚   â”œâ”€â”€ communication_overhead.png # Communication analysis
â”‚   â”œâ”€â”€ Forecasting_performance.pdf # Performance comparison
â”‚   â”œâ”€â”€ Forecasting_performance.png # Performance comparison
â”‚   â”œâ”€â”€ with_clustering.pdf     # Clustering impact analysis
â”‚   â”œâ”€â”€ with_clustering.png     # Clustering impact analysis
â”‚   â”œâ”€â”€ no_clustering.pdf       # No clustering baseline
â”‚   â”œâ”€â”€ no_clustering.png       # No clustering baseline
â”‚   â”œâ”€â”€ FedTime_Framework.pdf   # Framework architecture
â”‚   â”œâ”€â”€ FedTime_Framework.png   # Framework architecture
â”‚   â””â”€â”€ LLM_Architecture.pdf    # Model architecture details
â”‚   â””â”€â”€ LLM_Architecture.png    # Model architecture details
â”œâ”€â”€ ğŸ“ data_provider/                 # Dataset storage directory
â”‚   â”œâ”€â”€ data_factory.py
â”‚   â”œâ”€â”€ data_provider_init.py
â”œâ”€â”€ ğŸ“ exp/                     # Experiment runners
â”‚   â”œâ”€â”€ exp_basic.py           # Base experiment class
â”‚   â””â”€â”€ federated_experiments.py       # Federated experiment handler
â”œâ”€â”€ ğŸ“ layers/                  # Model layers and components
â”‚   â”œâ”€â”€ Transformer_EncDec.py  # Transformer encoder/decoder
â”‚   â”œâ”€â”€ SelfAttention_Family.py # Attention mechanisms
â”‚   â”œâ”€â”€ layers_init.py # Attention mechanisms
â”‚   â””â”€â”€ Embed.py               # Embedding layers
â”œâ”€â”€ ğŸ“ models/                  # Model implementations
â”‚   â”œâ”€â”€ FedTime.py             # Main FedTime model
â”‚   â””â”€â”€ __init__.py            # Package initialization
â”œâ”€â”€ ğŸ“ scripts/                 # Training and evaluation scripts
â”‚   â”œâ”€â”€ ablation_study_scripts.sh               # Ablation study
â”‚   â”œâ”€â”€ analyze_communications.py  # Communication Overhead
â”‚   â”œâ”€â”€ training_scripts.sh               # Training utilities
â”‚   â””â”€â”€ download_datasets.sh   # Dataset download automation
â”œâ”€â”€ ğŸ“ federated/              # Federated learning components
â”‚   â”œâ”€â”€ federated_client.py              # Federated client implementation
â”‚   â”œâ”€â”€ federated_server.py              # Federated server with clustering
â”‚   â”œâ”€â”€ federated_aggregation.py         # FedAvg, FedAdam, FedOpt
â”‚   â””â”€â”€ clustering_component.py          # K-means clustering utilities
â”œâ”€â”€ ğŸ“ utils/                   # Utility functions
â”‚   â”œâ”€â”€ utils_tools.py               # Training utilities and early stopping
â”‚   â”œâ”€â”€ utils_init.py               # Training utilities to initiate the script
â”‚   â”œâ”€â”€ utils_metrics.py             # Evaluation metrics (MSE, MAE, etc.)
â”‚   â””â”€â”€ utils_timefeatures.py        # Time feature engineering
â”œâ”€â”€ ğŸ“ main/                   # Core scripts
|   â”œâ”€â”€ run_longExp.py             # Main training script
|   â”œâ”€â”€ run_federated.py           # Federated training script
â”œâ”€â”€ ğŸ“ setup/                   # Experiment Setup
|   â”œâ”€â”€ requirements.txt           # Dependencies
|   â””â”€â”€ README.md                  # This file
```

</details>

---

## ğŸ”¬ **Reproducing Paper Results**

### **Complete Experimental Suite**

```bash
# Run all benchmark experiments
bash scripts/FedTime/federated_training.sh

# Individual dataset experiments
bash scripts/FedTime/ETTh1.sh    # Electricity Transformer Temperature
bash scripts/FedTime/Weather.sh  # Meteorological data
bash scripts/FedTime/Traffic.sh  # California traffic data
bash scripts/FedTime/Electricity.sh  # Power consumption data
```

### **Comprehensive Ablation Studies**

```bash
# Component ablation
python run_federated.py --use_clustering 0 --des 'NoClustering'
python run_federated.py --use_peft 0 --des 'NoPEFT' 
python run_federated.py --use_dpo 0 --des 'NoDPO'

# Hyperparameter sensitivity
python run_federated.py --num_clusters 2 --des 'Clusters2'
python run_federated.py --num_clusters 5 --des 'Clusters5'
python run_federated.py --peft_method lora --des 'LoRA'
```

---

## ğŸ—ƒï¸ **Supported Datasets**

| Dataset | Features | Timesteps | Granularity | Domain |
|---------|:--------:|:---------:|:-----------:|:------:|
| **ETTh1/h2** | 7 | 17,420 | 1 hour | âš¡ Energy |
| **ETTm1/m2** | 7 | 69,680 | 15 min | âš¡ Energy |
| **Weather** | 21 | 52,696 | 10 min | ğŸŒ¤ï¸ Weather |
| **Traffic** | 862 | 17,544 | 1 hour | ğŸš— Transportation |
| **Electricity** | 321 | 26,304 | 1 hour | ğŸ  Smart Grid |

---

## ğŸ› ï¸ **Customization & Extension**

### **Adding Custom Datasets**

```python
# 1. Create data loader in data_provider/data_loader.py
class Dataset_Custom(Dataset):
    def __init__(self, root_path, data_path, flag='train', ...):
        # Your custom dataset implementation
        pass

# 2. Register in data_provider/data_factory.py
data_dict['your_dataset'] = Dataset_Custom

# 3. Run training
python run_federated.py --data your_dataset --model FedTime
```

### **Implementing New Aggregation Methods**

```python
# Add to federated/aggregation.py
class YourAggregator(FederatedAggregator):
    def aggregate(self, client_updates, client_weights, global_params):
        # Your aggregation logic
        return aggregated_params

# Use in training
python run_federated.py --aggregation_method your_method
```

---

## ğŸ“Š **Citation**

If you find FedTime useful for your research, please cite our paper:

```bibtex
@incollection{abdel2024federated,
  title={A federated large language model for long-term time series forecasting},
  author={Abdel-Sater, Raed and Ben Hamza, A},
  booktitle={ECAI 2024},
  pages={2452--2459},
  year={2024}
}
```

---

## ğŸ“„ **License**

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ”— **Related Work**

| Paper | Description | Link |
|:------|:------------|:----:|
| **PatchTST** | A Time Series is Worth 64 Words | [![GitHub](https://img.shields.io/badge/GitHub-black?logo=github)](https://github.com/yuqinie98/PatchTST) |
| **LLaMA** | Large Language Model Meta AI | [![GitHub](https://img.shields.io/badge/GitHub-black?logo=github)](https://github.com/facebookresearch/llama) |
| **QLoRA** | Efficient Finetuning of Quantized LLMs | [![GitHub](https://img.shields.io/badge/GitHub-black?logo=github)](https://github.com/artidoro/qlora) |

---

<div align="center">

**â­ Star this repository if you find it helpful!**

[![GitHub stars](https://img.shields.io/github/stars/Jagoul/FedTime-A-Federated-Large-Language-Model-for-Long-Term-Time-Series-Forecasting?style=social)](https://github.com/Jagoul/FedTime-A-Federated-Large-Language-Model-for-Long-Term-Time-Series-Forecasting/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/Jagoul/FedTime-A-Federated-Large-Language-Model-for-Long-Term-Time-Series-Forecasting?style=social)](https://github.com/Jagoul/FedTime-A-Federated-Large-Language-Model-for-Long-Term-Time-Series-Forecasting/network/members)

---

*"Privacy-preserving collaborative intelligence for the future of time series forecasting"*
</div>
